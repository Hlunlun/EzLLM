{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: ['MTMAHFVEKYEAGTKPAICSNMFFVRLVDAGVPKDQIEYLGERLGIYPPLQVIQEAIQKEDTIEDAVGKTVSIEGGKLKGNQYKKTMESLVEIASYIKKS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from evodiff.pretrained import OA_DM_640M\n",
    "\n",
    "checkpoint = OA_DM_640M()\n",
    "model, collater, tokenizer, scheme = checkpoint\n",
    "\n",
    "model.load_state_dict(torch.load('models/epoch1_lr0.0001_accum8_warmup20_weight-decay1e-05/model_epoch0_accu36.5882.pth'))\n",
    "\n",
    "\n",
    "from evodiff.generate import generate_oaardm\n",
    "\n",
    "seq_len = 100\n",
    "tokeinzed_sample, generated_sequence = generate_oaardm(model, tokenizer, seq_len, batch_size=1, device='cpu')\n",
    "print(\"Generated sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detergent_gap_depths: [ 382  382  382 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  847  847  847\n",
      "  847 2015 2015 2015 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645\n",
      " 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645 1645  847  847  847\n",
      "  847  847  847  847  847  847  847  847  847  847  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968  968  968  968  968 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 2015\n",
      " 2015 2015 2015 2015 2015 2015 2015  382 1645  968  968  968  968  968\n",
      "  968  968  968  968  968  968  968  968 2015 2015  847  847  847  847\n",
      "  847  847  847  847  847  847  847  847  847  382]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "root = '../../data'\n",
    "data_dir = os.path.join(root,'ec_species')\n",
    "a3m_dir = os.path.join(data_dir, 'scaffolding-msas')\n",
    "files = [file for file in os.listdir(a3m_dir) if file.endswith('.a3m')]\n",
    "\n",
    "# the length of sequenc in .a3m\n",
    "detergent_depths = np.array([], dtype=int)\n",
    "#TODO I'm not sure but it may be the maximum ammount of gap in .a3m\n",
    "detergent_gap_depths = np.array([], dtype=int)\n",
    "# the ammount of sequences in .a3m\n",
    "detergent_lengths = np.array([], dtype=int)\n",
    "\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    input_file = os.path.join(a3m_dir, filename)\n",
    "    records = list(SeqIO.parse(input_file, 'fasta-2line'))\n",
    "    length = len(records[0].seq)\n",
    "    depth = len(records)        \n",
    "    gap_depth = max([len(record.seq) - record.seq.count('-') for record in records])\n",
    "\n",
    "\n",
    "    detergent_depths = np.append(detergent_depths,depth)\n",
    "    detergent_gap_depths = np.append(detergent_gap_depths,gap_depth)\n",
    "    detergent_lengths = np.append(detergent_lengths,length)\n",
    "    \n",
    "\n",
    "np.savez(os.path.join(a3m_dir,'detergent_depths.npz'), array=detergent_depths)\n",
    "np.savez(os.path.join(a3m_dir,'detergent_gap_depths.npz'), array=detergent_gap_depths)\n",
    "np.savez(os.path.join(a3m_dir,'detergent_lengths.npz'), array=detergent_lengths)\n",
    "\n",
    "\n",
    "# print(f'detergent_gap_depths: {np.load(os.path.join(a3m_dir,\"detergent_gap_depths.npz\"))[\"array\"]}')\n",
    "# print(f'detergent_depths: {np.load(os.path.join(data_dir,\"detergent_depths.npz\"))[\"array\"]}')\n",
    "# print(f'detergent_lengths: {np.load(os.path.join(data_dir,\"detergent_lengths.npz\"))[\"array\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence_models.constants import PROTEIN_ALPHABET, PAD, GAP\n",
    "from sequence_models.utils import parse_fasta\n",
    "from torch.utils.data import Dataset\n",
    "from evodiff.utils import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import hamming, cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3MDataset(Dataset):\n",
    "    \"\"\"Build dataset for A3M data: MSA Absorbing Diffusion model\"\"\"\n",
    "\n",
    "    def __init__(self, selection_type, n_sequences, max_seq_len, data_dir=None, min_depth=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            selection_type: str,\n",
    "                MSA selection strategy of random or MaxHamming\n",
    "            n_sequences: int,\n",
    "                number of sequences to subsample down to\n",
    "            max_seq_len: int,\n",
    "                maximum MSA sequence length\n",
    "            data_dir: str,\n",
    "                if you have a specified data directory\n",
    "        \"\"\"\n",
    "        alphabet = PROTEIN_ALPHABET\n",
    "        self.tokenizer = Tokenizer(alphabet)\n",
    "        self.alpha = np.array(list(alphabet))\n",
    "        self.gap_idx = self.tokenizer.alphabet.index(GAP)\n",
    "\n",
    "        # Get npz_data dir\n",
    "        if data_dir is not None:\n",
    "            self.data_dir = data_dir\n",
    "        else:\n",
    "            raise FileNotFoundError(data_dir)\n",
    "        \n",
    "        [print(\"Excluding\", x) for x in os.listdir(self.data_dir) if x.endswith('.npz')]\n",
    "        all_files = [x for x in os.listdir(self.data_dir) if x.endswith('.a3m')]\n",
    "        all_files = sorted(all_files)\n",
    "        print(\"unfiltered length\", len(all_files))\n",
    "\n",
    "        print(len(np.load(os.path.join(data_dir,'detergent_lengths.npz'))['array']))\n",
    "        print(len(np.load(os.path.join(data_dir,'detergent_depths.npz'))['array']))\n",
    "        print(len(np.load(os.path.join(data_dir,'detergent_depths.npz'))['array']))\n",
    "\n",
    "        ## Filter based on depth (keep > 64 seqs/MSA)\n",
    "        if not os.path.exists(os.path.join(data_dir,'detergent_lengths.npz')):\n",
    "            raise Exception(f\"Missing detergent_lengths.npz in {data_dir}\")\n",
    "        if not os.path.exists(os.path.join(data_dir,'detergent_depths.npz')):\n",
    "            #get_msa_depth_openfold(data_dir, sorted(all_files), 'openfold_depths.npz')\n",
    "            raise Exception(f\"Missing detergent_depths.npz in {data_dir}\")\n",
    "        if min_depth is not None: # reindex, filtering out MSAs < min_depth\n",
    "            _depths = np.load(os.path.join(data_dir,'detergent_depths.npz'))['array']\n",
    "            depths = pd.DataFrame(_depths, columns=['depth'])\n",
    "            depths = depths[depths['depth'] >= min_depth]\n",
    "            keep_idx = depths.index\n",
    "\n",
    "            _lengths = np.load(os.path.join(data_dir,'detergent_lengths.npz'))['array']\n",
    "            lengths = np.array(_lengths)[keep_idx]\n",
    "            all_files = np.array(all_files)[keep_idx]\n",
    "            print(f\"filter MSA depth > {min_depth}\", len(all_files))\n",
    "\n",
    "\n",
    "        # Re-filter based on high gap-contining rows\n",
    "        if not os.path.exists(os.path.join(data_dir,'detergent_gap_depths.npz')):\n",
    "            #get_sliced_gap_depth_openfold(data_dir, all_files, 'openfold_gap_depths.npz', max_seq_len=max_seq_len)\n",
    "            raise Exception(f\"Missing detergent_gap_depths.npz in {data_dir}\")\n",
    "        _gap_depths = np.load(os.path.join(data_dir,'detergent_gap_depths.npz'))['array']\n",
    "        gap_depths = pd.DataFrame(_gap_depths, columns=['gapdepth'])\n",
    "        gap_depths = gap_depths[gap_depths['gapdepth'] >= min_depth]\n",
    "        filter_gaps_idx = gap_depths.index\n",
    "        lengths = np.array(lengths)[[idx for idx in filter_gaps_idx if idx < (len(lengths) - 1)]]\n",
    "        all_files = np.array(all_files)[[idx for idx in filter_gaps_idx if idx < (len(all_files)-1)]]\n",
    "        print(f\"filter rows with GAPs > {min_depth}\", len(all_files))\n",
    "\n",
    "\n",
    "        self.filenames = all_files  # IDs of samples to include\n",
    "        self.lengths = lengths # pass to batch sampler\n",
    "        self.n_sequences = n_sequences\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.selection_type = selection_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "\n",
    "        def read_files(data_dir, filename):\n",
    "            \"\"\"\n",
    "            inputs:\n",
    "                data_dir : path to directory with data\n",
    "                filename: MSA name\n",
    "\n",
    "            outputs:\n",
    "                path: path to .a3m file\n",
    "            \"\"\"\n",
    "            if os.path.exists(os.path.join(data_dir, filename)):\n",
    "                path = os.path.join(data_dir, filename)\n",
    "            else:\n",
    "                raise Exception(\"Missing filepaths\")\n",
    "            return path\n",
    "\n",
    "\n",
    "        path = read_files(self.data_dir, filename)\n",
    "        parsed_msa = parse_fasta(path)\n",
    "\n",
    "        aligned_msa = [[char for char in seq if (char.isupper() or char == '-') and not char == '.'] for seq in parsed_msa]\n",
    "        aligned_msa = [''.join(seq) for seq in aligned_msa]\n",
    "\n",
    "        tokenized_msa = [self.tokenizer.tokenizeMSA(seq) for seq in aligned_msa]\n",
    "        tokenized_msa = np.array([l.tolist() for l in tokenized_msa])\n",
    "        msa_seq_len = len(tokenized_msa[0])\n",
    "\n",
    "        if msa_seq_len > self.max_seq_len:\n",
    "            slice_start = np.random.choice(msa_seq_len - self.max_seq_len + 1)\n",
    "            seq_len = self.max_seq_len\n",
    "        else:\n",
    "            slice_start = 0\n",
    "            seq_len = msa_seq_len\n",
    "\n",
    "        # Slice to 512\n",
    "        sliced_msa_seq = tokenized_msa[:, slice_start: slice_start + self.max_seq_len]\n",
    "        anchor_seq = sliced_msa_seq[0]  # This is the query sequence in MSA\n",
    "\n",
    "        # slice out all-gap rows\n",
    "        sliced_msa = [seq for seq in sliced_msa_seq if (list(set(seq)) != [self.gap_idx])]\n",
    "        msa_num_seqs = len(sliced_msa)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if msa_num_seqs < self.n_sequences:\n",
    "            print(\"before for len\", len(sliced_msa_seq))\n",
    "            print(\"msa_num_seqs < self.n_sequences should not be called\")\n",
    "            print(\"tokenized msa shape\", tokenized_msa.shape)\n",
    "            print(\"tokenized msa depth\", len(tokenized_msa))\n",
    "            print(\"sliced msa depth\", msa_num_seqs)\n",
    "            print(\"used to set slice\")\n",
    "            print(\"msa_seq_len\", msa_seq_len)\n",
    "            print(\"self max seq len\", self.max_seq_len)\n",
    "            print(slice_start)\n",
    "            import pdb; pdb.set_trace()\n",
    "            output = np.full(shape=(self.n_sequences, seq_len), fill_value=self.tokenizer.pad_id)\n",
    "            output[:msa_num_seqs] = sliced_msa\n",
    "            print(\"msa num_seqs < self.n_sequences, indicates dataset not filtered properly\")\n",
    "            # raise Exception(\"msa num_seqs < self.n_sequences, indicates dataset not filtered properly\")\n",
    "        elif msa_num_seqs > self.n_sequences:\n",
    "            if self.selection_type == 'random':\n",
    "                random_idx = np.random.choice(msa_num_seqs - 1, size=self.n_sequences - 1, replace=False) + 1\n",
    "                anchor_seq = np.expand_dims(anchor_seq, axis=0)\n",
    "                output = np.concatenate((anchor_seq, np.array(sliced_msa)[random_idx.astype(int)]), axis=0)\n",
    "            elif self.selection_type == \"MaxHamming\":\n",
    "                output = [list(anchor_seq)]\n",
    "                msa_subset = sliced_msa[1:]\n",
    "                msa_ind = np.arange(msa_num_seqs)[1:]\n",
    "                random_ind = np.random.choice(msa_ind)\n",
    "                random_seq = sliced_msa[random_ind]\n",
    "                output.append(list(random_seq))\n",
    "                random_seq = np.expand_dims(random_seq, axis=0)\n",
    "                msa_subset = np.delete(msa_subset, (random_ind - 1), axis=0)\n",
    "                m = len(msa_ind) - 1\n",
    "                distance_matrix = np.ones((self.n_sequences - 2, m))\n",
    "\n",
    "                for i in range(self.n_sequences - 2):\n",
    "                    curr_dist = cdist(random_seq, msa_subset, metric='hamming')\n",
    "                    curr_dist = np.expand_dims(np.array(curr_dist), axis=0)  # shape is now (1,msa_num_seqs)\n",
    "                    distance_matrix[i] = curr_dist\n",
    "                    col_min = np.min(distance_matrix, axis=0)  # (1,num_choices)\n",
    "                    max_ind = np.argmax(col_min)\n",
    "                    random_ind = max_ind\n",
    "                    random_seq = msa_subset[random_ind]\n",
    "                    output.append(list(random_seq))\n",
    "                    random_seq = np.expand_dims(random_seq, axis=0)\n",
    "                    msa_subset = np.delete(msa_subset, random_ind, axis=0)\n",
    "                    distance_matrix = np.delete(distance_matrix, random_ind, axis=1)\n",
    "        else:\n",
    "            output = sliced_msa\n",
    "\n",
    "        output = [''.join(seq) for seq in self.alpha[output]]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding detergent_depths.npz\n",
      "Excluding detergent_gap_depths.npz\n",
      "Excluding detergent_lengths.npz\n",
      "unfiltered length 934\n",
      "934\n",
      "934\n",
      "934\n",
      "filter MSA depth > 64 930\n",
      "filter rows with GAPs > 64 929\n",
      "929\n"
     ]
    }
   ],
   "source": [
    "from evodiff.utils import Tokenizer\n",
    "import os\n",
    "\n",
    "root = '../../data'\n",
    "data_dir = os.path.join(root,'ec_species/scaffolding-msas')\n",
    "n_sequences = 64\n",
    "min_depth = 64\n",
    "selection_type = 'MaxHamming'\n",
    "max_seq_len = 1024\n",
    "\n",
    "dataset =A3MDataset(selection_type, n_sequences, max_seq_len, data_dir=data_dir, min_depth=min_depth)\n",
    "train_size = len(dataset)\n",
    "\n",
    "random_ind = np.random.choice(train_size, size=(train_size - 10000 if train_size>1000 else train_size), replace=False)\n",
    "# print(\"TRAIN SIZE:\", train_size, random_ind)\n",
    "\n",
    "print(dataset.__len__())\n",
    "# print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before for len 5\n",
      "msa_num_seqs < self.n_sequences should not be called\n",
      "tokenized msa shape (5, 383)\n",
      "tokenized msa depth 5\n",
      "sliced msa depth 5\n",
      "used to set slice\n",
      "msa_seq_len 383\n",
      "self max seq len 1024\n",
      "0\n",
      "> \u001b[0;32m/tmp/ipykernel_85550/3851817186.py\u001b[0m(134)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    132 \u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    133 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 134 \u001b[0;31m            \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    135 \u001b[0;31m            \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmsa_num_seqs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msliced_msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    136 \u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"msa num_seqs < self.n_sequences, indicates dataset not filtered properly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sequence_models.collaters import MSAAbsorbingCollater\n",
    "from torch.utils.data import Subset\n",
    "from sequence_models.constants import MSA_ALPHABET\n",
    "\n",
    "# ds_train = Subset(dataset, random_ind)\n",
    "\n",
    "\n",
    "collater = MSAAbsorbingCollater(alphabet=MSA_ALPHABET, num_seqs=64)\n",
    "\n",
    "data = collater.__call__(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OmegaFold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
