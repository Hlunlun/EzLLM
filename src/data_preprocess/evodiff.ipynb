{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chialun/.conda/envs/evodiff/lib/python3.12/site-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
      "\n",
      "Due to the on going maintenance burden of keeping command line application\n",
      "wrappers up to date, we have decided to deprecate and eventually remove these\n",
      "modules.\n",
      "\n",
      "We instead now recommend building your command line and invoking it directly\n",
      "with the subprocess module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from Bio import AlignIO, SeqIO\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline\n",
    "from Bio.Seq import Seq\n",
    "from evodiff.pretrained import MSA_OA_DM_MAXSUB\n",
    "import tempfile\n",
    "from Bio import AlignIO, SeqIO\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline\n",
    "from Bio.Seq import Seq\n",
    "from evodiff.generate_msa import generate_query_oadm_msa_simple\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "root = '../../data'\n",
    "device = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Sequence of enzymes (.fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collat_data.ipynb 最後一個cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MSA (.a3m file)\n",
    "**conda environment: evodiff** \\\n",
    "since the model only accept .a3m file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = os.path.join(root,'uniprot/research/motif')\n",
    "\n",
    "pro_bar =tqdm( [file for file in os.listdir(os.path.join(directory, 'ec_num')) if file.endswith('.fasta')])\n",
    "for filename in pro_bar:\n",
    "    input_file = os.path.join(directory,'ec_num', filename)\n",
    "    a3m_file = os.path.join(directory,'msa',filename[:-6]+'.a3m')\n",
    "    fasta_file = os.path.join(directory,'msa',filename[:-6]+'.fasta')\n",
    "    \n",
    "    # os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "    # Read in fasta file\n",
    "    records = list(SeqIO.parse(input_file, 'fasta'))\n",
    "\n",
    "    if len(records) > 2:\n",
    "        pro_bar.set_postfix(message=f' Processing {filename}')\n",
    "        # Check max length of all sequences\n",
    "        max_length = max(len(record.seq) for record in records)\n",
    "\n",
    "        # pro_bar = tqdm(records, desc=f'Processing {filename}')\n",
    "        # Add padding\n",
    "        for record in records:\n",
    "            seq_length = len(record.seq)\n",
    "            if seq_length < max_length:\n",
    "                padding_length = max_length - seq_length\n",
    "                padding_seq = Seq('-' * padding_length)  # 將填充字符從空格改為短橫線'-'，這是多序列對齊中更常用的間隔符。\n",
    "                record.seq += padding_seq\n",
    "            \n",
    "            # time.sleep(0.1)\n",
    "\n",
    "        # Create a temporary file for the padded sequences\n",
    "        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.fasta') as temp_fasta:\n",
    "            SeqIO.write(records, temp_fasta, 'fasta')\n",
    "            temp_fasta_name = temp_fasta.name\n",
    "\n",
    "        # Run Clustal Omega\n",
    "        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.aln') as temp_aln:\n",
    "            clustal_omega_cline = ClustalOmegaCommandline(\n",
    "                cmd='clustalo', \n",
    "                infile=temp_fasta_name,\n",
    "                outfile=temp_aln.name,\n",
    "                verbose=True,\n",
    "                force=True\n",
    "            )\n",
    "            stdout, stderr = clustal_omega_cline()\n",
    "\n",
    "        # Read the alignment and write it as fasta\n",
    "        alignment = AlignIO.read(temp_aln.name, 'fasta')\n",
    "\n",
    "        AlignIO.write(alignment, fasta_file, 'fasta')\n",
    "\n",
    "\n",
    "        # Convert to A3M format manually\n",
    "        with open(a3m_file , 'w') as handle:\n",
    "            for record in alignment:\n",
    "                handle.write(f\">{record.id}\\n\")\n",
    "                sequence = str(record.seq).replace('.', '-')  # Replace '.' with '-'\n",
    "                handle.write(f\"{sequence}\\n\")        \n",
    "\n",
    "        # print(f'MSA has been written to {output_file }') \n",
    "    else:\n",
    "        pro_bar.set_postfix(message=f'{filename} contains 1 sequence, nothing to align')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional generation MSA: EC Number + Organism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from evodiff.conditional_generation_msa import \n",
    "\n",
    "root = '../../data'\n",
    "directory = os.path.join(root, 'uniprot/research/motif')\n",
    "\n",
    "input_files = [file for file in os.listdir(os.path.join(directory, 'ec_num'))]\n",
    "\n",
    "for input_file in input_files:\n",
    "\n",
    "    records = list(SeqIO.parse(input_file, 'fasta'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional generation MSA: EC Number + Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional generation: EC Number + Organism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Motif Logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "import logomaker as lm\n",
    "\n",
    "directory = os.path.join(root, 'uniprot/research/msa')\n",
    "pro_bar =tqdm( [file for file in os.listdir(directory) if file.endswith('.a3m')])\n",
    "for filename in pro_bar:\n",
    "    input_file = os.path.join(directory, filename)\n",
    "    output_file = os.path.join(root, f'uniprot/research/plots/motif_logo/{filename[:-4]}_logo.png')\n",
    "    csv_file = os.path.join(root, f'uniprot/research/plots/motif_csv/{filename[:-4]}_logo.csv')\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # extract ww domain sequences\n",
    "    seqs = [seq.strip().upper() for seq in lines if ('#' not in seq) and ('>') not in seq]\n",
    "    # create counts matrix\n",
    "    counts_df = lm.alignment_to_matrix(sequences=seqs, to_type='counts', characters_to_ignore='.-X')    \n",
    "    counts_df.to_csv(csv_file, index=False)\n",
    "\n",
    "\n",
    "    # filter base on counts\n",
    "    num_seqs = counts_df.sum(axis=1)\n",
    "    pos_to_keep = num_seqs > len(seqs)/2\n",
    "    counts_df = counts_df[pos_to_keep]\n",
    "    counts_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # show full ww counts\n",
    "    plt.figure(figsize=(3000, 3))\n",
    "    # logo = lm.Logo(counts_df)\n",
    "    logo = lm.Logo(counts_df)\n",
    "    \n",
    "    plt.title(f\"Motif Logo: {filename[:-3]}\")    \n",
    "    \n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')    \n",
    "    \n",
    "    pro_bar.set_postfix(message=f'Saved logo for {filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new MSA \n",
    "using EvoDiff to generate new MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (ec:3.2.1.1)AND(organism_name:Streptomyces strain A3)_reviewed0_2.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]/home/chialun/.conda/envs/evodiff/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 1024/1024 [00:55<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) AEHTEZEEKLZFKEFZZZFFZZLMFZZKSHSCIFTZEZZZZVZFFFFNEZZPZEZATEFEDAQELHZFLEGEZGSEEKEADYZREQZEGSTEZZPNNFERFZGDZALFFBZQZZTQENZAZSLQIEREDQVFFVYZVBKFEEZBZSZHSSFEEYGZLQVGBKGBMNMZSEGPFGZTPBZFFEYZSGZTBCKLLQNLBETFMZEFFKGTKLKYDFQTKEBEZPBEKBFNNTFCGTTBSEFBBFZQFHMRIFFFFSGQFZBZNYKQVEAFHGMILHZEGMTAHKFKDSHYBFLELTZZTZVNSTFTCZLTFEQZEEFHEZIGBHBYFRTZZKZJFGEVFKBZGZYBEGFZEBREZZKNFZZNGKLYGHZZZBBGKZEGZEBLCZGEZZZLZBFTFFBHBSZFFKZVGNEPECZKZLEZFZDKBZFZZLBBFEBGBSGFLFBFEBSEZLZIVKYTFBDHSKQZDGZCMZGBYZGGZZLBFKIZZGQDFGSVGKEAEEFTYEFVZQBNFAGVZBRZYZNFKMZIRVZWZBQPAEYBAFZYGAONFZFEVVBZEZRZZFEZKLNZBBBFFGQZYKZZFEFZZZLFZLZKKZAEYZACLZQKYZKVGZZZSLKLLZBZFPNFKSBZZZZNZLFZZKZZZZFLRZYHHZGZZFBVFZTKYZTZZFNZFZZZZZHVFFQIFRYKFVKAFSZIFVZZEFZZGFZDZKFZYZZWZKZKKBEEKYRZFRZBKZZGFSZZFFTQZPZEZZNKGGNFZFBFZSFKZZABFFPZLHFBYFFZIQSZZZZFFANZNEHQZRZZZSEHFSZZFTNKZFAEBFEZZEFEZZQGEFZZQFZZZNPAATZTFGZZASTLFZDZBZFFZZMZGFFBHZFEZYEVBZEKKHAABZVZBVZDBNIABTBZQZBENKFSZEALZAGZRZZLZFFZFZRAGLZHEHLZFKLDQBZZCDZGIZFEEGHFBGCYZZZLZNFZFBZKVEZPSSLZIFBVFZFGBKFBZETZZILZYFZLBNERZDEFZIEZFTFYZTZVEZIZBZGZPGHR\n",
      "\n",
      "Processing (ec:3.1.1.3)AND(organism_name:Bacillus subtilis strain)_reviewed0_8.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:02<00:00, 75.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) MNFNZKEZZZVFAFNGZSFLDFZJZGGZZGZLZZZNADKZZEVZHGGEVSEGNLINKIZZSVGEALTANTQZEVBALAINFWHIINZLSZVEZGZSAGLZGKKZFZALZGNAZAKVHZNNVFEREZHZKZTTEFTKIFEKAKVDLKKDVIRAIEZELZZEZVKGKVNHLKFEZEGVKIAKFZFAGTFLAVESEETGQZEZBVEGLZMZZAGDKZ\n",
      "\n",
      "Processing (ec:3.1.1.3)AND(organism_name:Candida albicans (C. albicans))_reviewed0_48.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 985/985 [00:47<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) IAFZKZZQGNSHPGABFSABEWYTAEEGELGFKZNLZEZNTZFVLKTNAASDTZEKHMZNZEMZZTEGEZZFLBSYEZKYTVZENFNBEVZVTVTTTZVLLGVBNHBDZCBNFEZFZGDDTRABEZSCTBTESELIKEKZVNKAELCBCGLZZFBBBLBVFGFZPFEATKQFKLTKTYMFKSFVNZZOATYBLABZMBRNQLBSGQFZFABZGTILQZBQVKBLNZAMZFTGIBLAMBTGZLDGZEZDEBFTIZRZHBTDDBSQZREEHDYFLAABAZBLFMNEEBIEFYZNGGDSCVEQZVAKKAZCGBLTZZFIFNZGAAYOTFHYYBZEITBMLLZGVRZZLKNNWFTGMMZEZKTEQNIRVGBQFTELRVNVDZLGBZFEKKNZSANQTZAMGCBDNTZKBKFVZNZFLKBHVPSSKFJBYMVZIAFKGHBAGZKHCNLZZENNNBEVNEYNZKLSNTTEDTZZLLZYVYAZEKADYFBFNZBTZQKZGANPZFESBBVGBZKTZIGTLKRZYNZBLZZFZKOGZFSAZZEZTFRALFTZZZBGIBTHNZSZZKZZEGEFNAZKFEVLZXIEKFKZZLGZTGFSSNBZBZEETLAEBZIBFLAZEIAMYZKGZZKFEIFQYYRASEAZFEFBAAAZBATGHVBZFHFMQPTLRZZZZZFEQZFSBFZFAZNZFYLBFEZNZZVZVHZHNNEBBATEAMBBAFSTGLTZBKRZIZLBZZNNZKEZFTBZKKNZLLZLFZHQZMBLGZZTESFFNZVZBZZLHESBQYBZQICFYLGZSZHARSFEINLYEZELZAZKRZBGTZGZZZHIRKRZZZVBVTZZDBBEKZLDNQFKZBZMLVHKBZZZZZTTBYYFZTZPNOFZBDNKTHFNYZZZAVAEELPFFIZEZZCZFWZZSZRBZNHTELZABFEGZJZBZZIALZBFZETZZYZMLSFFBBFVYZZQFVZLRHVRGZSZHEZFZMZZVTBZZGTYEMVTYEMBFZAZZ\n",
      "\n",
      "Processing (ec:3.1.1.3)AND(organism_name:Bacillus subtilis strain)_reviewed1_2.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [00:03<00:00, 55.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) GAZVKKZFVTELTPJFZZTVZBYZVVEDZKTEERTSTZTZJFETTIZZVFVJSGZNTQLZVFRHTKJTQVHZFZZCZJAKFTEJZDLZZZKJYTAZTDBVAJAJCVRGZJGJVJFVNJZKZVKTKZGNYADZJAVFJZJAQZCTZSBZWZLZDTYZTALJTZQJZZPALZNVBJZSTKEAGKZKTZGEOZKFYJZTHTKZVKAZNLQZJZPZ\n",
      "\n",
      "Processing (ec:3.1.1.3)AND(organism_name:Candida albicans (C. albicans))_reviewed1_11.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:26<00:00, 22.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) MFNZZGZZSLBTZEZZZBRKZBFZFATGVZNFFVLCQTVEFAABZLRIDGNTTABBFRZZLZVBAWPKBVGLHLNZCRZVLGZSETTAQNBRTCTFZBEZNZTANNIBVYTFNKZLBBAZAGNBTBLLFNHAKILKPKFBRAENHGHHMTZFBCAYZHLNNDIGNEERZABBIESINZANNEBZZIHAEESBZVBTTNGIZYSAZCQLTZVYTVSZTSFTNGGZTMTETKZFCBRIZLBGLBNSMVZFKZNSTBINBEEETVNMZNKTNFBNSVFTADBTITZZZQZVBENTZVFCENHZFFAZGEZGZBYKITTTBELCKCLBZALASFZEBVTDKEADBZBFZTKALNTAFKZGSLJTDBZGVKVBABNEBELNNBBFBTAZVZEIVRFKPIZTBRJSCNZSNZSZZTABCZZGZTAZKAFKVTTTMNQSKBTHSFGYQTBFFKZZFNICZFBBBNEBLIBTTAIBJZSISPTAZFSBDAEKDZYZKEFLIGZANSZTZKATTVBGIVNEFBRSNBZMILFZFTMFJBZKZLZTZCZZFZZTZZNZKKZZYDVKZGMZZFZPTVDEABZKCYGQBHTZTNIZZKZSJTVZFALMEZVGZVSB\n",
      "\n",
      "Processing (ec:3.4.21.62)AND(organism_name:Bacillus licheniformis)_reviewed1_2.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 379/379 [00:14<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) MGNENKNYTFGAAWFZNSBKEKYEADZYGQYVGNEWGSFFZZEFCEKIYAZNZEILLOKBBFELEGZWAEIYHYWDNEEFGAFILVQRWCZCBFVYFHYDFEFEFZAMYQEYCBQFBGBZGINKYANYHYTGYDYGBAAZFIWJECZHYAEGBEYGLFNEIYGINQBAKYNZYGDNEPLNEPIYNGECZFFYQQFAESEEAPAVFWLLESKZQLCFYEVACEAQBEFYKAPFNNTFKGQFBEKLBEYHYMYVHLFYHIKFKGRNYGCEFYAIFEELBYVIFIELPNVZBFQGCBRHYGIYPKAEQYYFESLAIFJYTZGKEKLBZDDZIFAZFLYFHSFYLYPGTFNEKTDHBBFFLYAZEELTYARJMKAFVDFEKFI\n",
      "\n",
      "Processing (ec:3.2.1.78)AND(organism_name:Lentinula edodes)_reviewed0_3.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 788/788 [00:53<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) ZMTMFZZPFSVZHINIAKLBMAHAZBTZZBGEERTNKATEFCSFKNZZCFAZHEVGKLTZZZLATGSZNGYNRYEFZHHEZCEATGILZEZZYYKLDBEZOZFZFKAYZEQZZLZGZFZLBKZZZTBTETSBBZRZZFTHAZZZLTKZKZTZFBFHLCZBZMBAYZZKTBLTZEZLMANZCKBNNFETFBVFFZBEOOJZFHYBGBZFEHBZNZFRAGMZNLBENFHMFZBCRFNIZZKRTFFFBZZAKFLBZVABYFKVFMNAZTLZEVKREFECZLLNLBFBZZHGYAHVKZZHZZNBKBBFAZNKFHFZZBAVAHRGYELNSZVAZNBADZSGZQAVLKNCYZOTMEZOKMKYLAMZBTSDGCQPTPNDZZNNFBBFQEZSZALZLZBCZSFELZMAEBEBVYZBKGIGZFANZRZZZQGKZEHFSMBZHQBSNGQZYTPRSDLZGLMERQOFFCTZOOZZTEBZFQELMBGREAOIHZZFSZVKAOZZBVZAHEZLZBARZHEQEVZRZZAZBLZLSGZFZFYFZZAZINBSLRAZZFZZTZGZZZPZZKAPKZFVFSZZBRZZTDFRAVKZGDITTSZZZTNHECFZZZZFQLTZHEFAZZZZBZMGFQATZZBZZZPZAZZGZEKFZZYHHRZYZEAFZKBIKHFSZZAFHBZZTAKZHZZZKZAATKHTZFCAFBZGCEVDIQZCZZKZENTFZCZKZSZFFTWNTBAZZHHZRSHDCYZFAZGZNDZJGEZZKZFHKZYZFKHWZZAHFZFZFZZZNFZYZVZZDLSFSQSLZZZQMAAL\n",
      "\n",
      "Processing (ec:3.2.1.4)AND(organism_name:Fibrobacter succinogenes S85)_reviewed0_19.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [01:30<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) MTZHPZZZAZHITKZGVBWBFTMKAEAZIZBZZZSBZZZBLZHAHBBITHGFZINHDASBTQZZZBVKSAZNZGILLFNZZFMQBKZAFSHZETSFFBZHBKZBZGQBFBTNAGZFAITDFBASZYIZKZHQQGKKGKVTMHBFBFYDZZKBENTLBBEFEMBNZFTKZZZMGELLHKTZZHBSZEGNZGNTNTKEANMGVEAZVTBZBHZTEVTZNMZZYBLKEZNTNQZCETRVZINGZNLKBSKBKLFAZHZZFYKEJLBKKALFKZBZFTVZBFHLFYNNZBNTSZZBNLZGIBTFKEVBFTBZBHZHKEZTZBBTENTFZAZAQGKZSTNGHRFGTEIFELFHBTIEZGLKTNRTBENDGBFBHNHKTBZGSIBKEHTZWYHREETBFTDEAKZSZPFSNKFFIAZOYEZZLQBHBBKHYZABTHTFYYFTLNHLBCTTFKNBFVTFHGBZBSRFDEPZFRIBGFNFZGBFZZQZQCFRNBZEFZATLBHTZBEVRHLBFOZNDEMZINBSZSYZNZAYZBMHZBLZZKSNIZCKZZFTZYNZYGZZLAGTLFFZZEHHEIAZZLZBWZZTBELHHEZZZCEHIPAZLNZTBZVRFNNLRYZFBZFBGEZBEZFASZZZTZRZZONFFAANVFMSZZNFHTZFETZTTZLFTDFBFHRLZSZZEZFZNZZZKRSTZFZVSEIZZFZFKGHSFZGKSZHZHBKDVFZZTZEZEZFHZNITSFZZBYKTZFZFNZHFZHTZZEZZTTAZZMBAFZLVMKTTNREFTZZLFSZSKZDKBZSHKAZMTCDFFFZZQFETFZFHHZZFINZZFZAEZGZZGYGZANRFZKBEFBNZZNTPZCLPGAZBZZETATZBSBKZZZZNLVIKLZNVDFEZZZZVENYSATTVQKKTAZZZZFZEZTHHBRKFZYGLZBRZKYGKCZZZHTCTHZKBSFANBKYRGDFZTCZDZLZNKZHZZFVEEFZIFEAEZARKZFLCNRZNEZEAHEZHHBZVEZPZKZZZFHTFKKZZZZTFBEFTBZKBZMDE\n",
      "\n",
      "Processing (ec:3.2.1.4)AND(organism_name:Paenibacillus sp. IHB B 3084)_reviewed0_3.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:26<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) MDBEHKEEBGFEVRFZZBYAYQFLSFEBHQFLLFVLRTATRAQFLLVKLINAZZBLEQVZKVKCYZGNTPHDGJCEAZYKFBEBGEEFNLALNQNBTLKSDMNIFFLDAVFNPEEPEBECPFLFFFGLIFLNBABVZANGLFTEZEEZEVNDDBARFBEBVPENYGFENKPNFDYKLNSNNZAKVHGFVRKENEKIZAHEYLARDBVGIZVKAFCFHVGNIGBYGEQBGLFGGNEKKQENDTHYDRHNKLGNBAIFDCZGFFFZVVKGRFTGELPEGMLAELEKFENLCCBZZATENCEEFBNAKGLDEKGSFFLDFEHNIELKKKEBGDAEFNFBGIQFNVHKVVVDFZLLEKIRHTLEENNNIFVBYGLFFBGEZNFHEQFEENKLKNEADNVELNVBNGBANVALEENFZSPFENHFKKNDBZDFGGLKZKNKALKRHNZLKZKZFBNKFLALGQCNABANCTDTEGPTKLABZAFFFNVFCFRENHEKEFFIZRBVSVLEVEWILEFAVFVNZCTENLTNZBFTLQFNGSFKK\n",
      "\n",
      "Processing (ec:3.1.1.3)AND(organism_name:Acinetobacter sp.)_reviewed0_22.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:24<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) MZZKHZHNATZZANIZFBZNFKGLVCBPYFZZAKBKKBKFKGAGNRKARBZKLZHNOGNHKZOZTHAAYZQSFZVBZZVZZSZAQTLJYKTEINIZGLFROHKLFAZAZESSZYNGPTVKZAKENNNTNHNOGZZAFKAOFVDBNKFYYBNKGFNNLBNMZFZQNCANFZDSBNGLBABZRGEBDCEQBMASZBNFKCZYKBGKEFPZZIZYLEQIBZZYZAFGKVGEZNZNLLEKMZCBFZVGLWWENYPHLOVSALSQSBEBTGLSDEGFGZZGZBQKTBBAZBAFELFJNYGHINFKKLSOLTYZTHGAZFWFSKPGNTZFEBZBIBIELAFBZCBSOHSHZLAKZQTIZBDGVNPLMOZELFAKVEQKZEZZZNSFIZGTFEOFAANFZFBELBAZZZENBZKLZZNEDLLILZZONSBOALQKTZZFRZKFBNZQNFHTZBISVGHTDZHNDIZVLIFEAIZVZMKGZAFNQJGZKGFRSGGTIVZHHGHQQDZGZEKEYEYNZNZFPVTLRSFZNMZZCZKAF\n",
      "\n",
      "Processing (ec:3.4.21.62)AND(organism_name:Bacillus licheniformis)_reviewed0_3.a3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:09<00:00, 40.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sequence (no gaps, pad tokens) KGKDZRZVTZMLEAKFBSDGZZVNLFILMMSZZZFSZDKLVBZKSKECILLZEFAFZSZIZVVYZIZLZEZCZZVARZKODAZMZZLETZETZKZKOTZZSBTADDEZSATJZIZDBIANZKZTTBKZETTLKQARBTEZDEHKTTTBKZBVZKZKZZZAADQKEZOAVEKAAGTTKZZSGZILZZBIZZASVAVZZVVZAZSBMAZZBZQCLGNZTWKZBAZNZQGZGTNITLTBRFVADIDSLSZBMBVAAGVZZSZZBDZZKEZEDZBAZAVGZZFVZZZVATLASLTGTDZMTZMZSNZBZGBIZKLLZZELTBLAZATCVDAQTZRAZTZKZZSTLZVZZZZDZLZREAVDKEZLMLOZKEZSZEZZNTEVAVKZFZ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from evodiff.generate_msa import generate_msa\n",
    "from evodiff.generate_msa import *\n",
    "from evodiff.pretrained import MSA_D3PM_UNIFORM_MAXSUB\n",
    "\n",
    "checkpoint = MSA_OA_DM_MAXSUB()\n",
    "# checkpoint = MSA_D3PM_UNIFORM_MAXSUB()\n",
    "model, collater, tokenizer, scheme = checkpoint\n",
    "\n",
    "directory = os.path.join(root,'uniprot/research/msa')\n",
    "all_files =  [file for file in os.listdir(directory) if file.endswith('.a3m')]\n",
    "\n",
    "for filename in all_files:\n",
    "    print(f'Processing {filename}')\n",
    "    msa_file = os.path.join(directory, filename)\n",
    "    n_sequences=1 # number of sequences in MSA to subsample\n",
    "    records = list(SeqIO.parse(msa_file, 'a3m'))\n",
    "\n",
    "    if len(records) > 1:\n",
    "        # pro_bar.set_postfix(message=f' Processing {filename}')\n",
    "        # Check max length of all sequences\n",
    "        max_length = max(len(record.seq) for record in records)\n",
    "\n",
    "        \n",
    "    # seq_length=256 # maximum sequence length to subsample\n",
    "    seq_length=1024 if max_length>1024 else max_length # maximum sequence length to subsample\n",
    "    selection_type='random' # or 'MaxHamming'; MSA subsampling scheme\n",
    "\n",
    "\n",
    "    tokeinzed_sample, generated_sequence  = generate_query_oadm_msa_simple(msa_file, model.to(device), tokenizer, n_sequences, seq_length, device=device, selection_type=selection_type,)\n",
    "    # tokeinzed_sample, generated_sequence  = generate_msa(msa_file, model.to(device), tokenizer, n_sequences, seq_length, device=device, selection_type=selection_type,)\n",
    "    \n",
    "\n",
    "    print(\"New sequence (no gaps, pad tokens)\", re.sub('[!-]', '', generated_sequence[0][0],))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------MKLVNIWL---LLLV-VLLCGKKH-LGD--RLEK----------------------KSFEKAPCPGCSHLTLKVEFSSTVVEYEYIVAFNGYFTAKARNS-------------FISS--ALK--SSEVDNWRIIPRNNPSS------------------DYPSD----------FEVIQIKEKQKAGL-LTLEDHPNIKRVTPQRK--------VFRSLKYAES--D----------P--------TVPCNETRWSQK-W---QSSRPL-RRASLSLGSGF--------WHATGRH-SSRRLLRAIPRQVAQTL--------QADVLWQMGYTGANVRVAVFDTGLSEKHPHFKN---V-------KER-------------TNWTN------------------------------ERTLDDGLGHGTFVAGVIASM------------RECQGFAPDAELHIFRVFTNNQ---VSYTSWFLDAFN--YAILKKIDVLNLSIGGPDFMDH-----PFVDKVWELTA--NNVIMVSAIGNDGPLYGTLNNPADQMDVIGVGGID-----------------------------------------------------------------------------------------------------------------------------------------------------------FED----------NIARFSSRGMTTWELP-----GGYGRMKPDIVTYGAGV----RGS------------GVKGGC-RALSGTSVASPVVAGAVTLLVSTVQ--KR-ELVNPASMKQALIASARRLPG---------------VNMFEQGHGKLDLLRAYQILNSYKPQASL-SPSYIDL----------------------TECPYMWPY-CSQPIYYGGMPTVVNVTILNGMGVTGRIVDKPDWQPYLPQNGDNIEV-----------AFSYSSVLWPWSGYLAISISVTKKAASWEGIAQGHVMITVASPAETESKNGAEQTSTVKLPIKVKIIPT----------PPRSKRVLWDQ----YHNLRYPPGYFPRDNLRMKNDPLDWNGDHIHTNFRDMYQHLRSMGYFVEVLGAPFTCFDASQYGTLLMVDSEEEYFPEEIAKLRRDVD-NGLSLVIFSDWYNTSVMRKVKFYDENTRQWWMPDTGGANIPALNELLSVWNMGFSDGLYEGEFTLANHDMYYASGCSIAKFPEDGV-VI--TQTFKDQGLEVLKQ--ETAVVENVPILGLYQIP--AEGGGRIVLYGDSNCLDDSHRQKDCFWLLDALLQYTSYGVTPPSLSHSGNRQ-RPPSGAGSVTPERMEGNHLHRYSKVLEA-HL--GD----------------PKPRPLPACPRLSWAKPQPLNETAPSNLWKHQKLLSIDLDKVVLPNFRSNRPQVRP-LSPG------ESGAWD-IPG--GIMPGRYNQEVGQTIPVFAFLGAMVVLAFFVVQINKAKSRPKRRKPRV-KRPQLMQQVHPPKT-------PSV\n",
      "sp|Q14703|MBTP1_HUMAN\n",
      "sp|Q14703|MBTP1_HUMAN\n",
      "<bound method SeqRecord.format of SeqRecord(seq=Seq('------------------------------------------------------...PSV'), id='sp|Q14703|MBTP1_HUMAN', name='sp|Q14703|MBTP1_HUMAN', description='sp|Q14703|MBTP1_HUMAN', dbxrefs=[])>\n"
     ]
    }
   ],
   "source": [
    "print(records[0].seq)\n",
    "print(records[0].name)\n",
    "print(records[0].description)\n",
    "print(records[0].format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MZGTZZZZZERZAKYYAZZFEVAWQNZWLHAKEDYVFZHOLZAEEMKKAKEZLBEGKGEIEVAQDDEALRZLEALZTNAEZZKZEYYLAAFYKZQMAELAEZQZTPAEKFVOELFOKEVAZFAZGNFQDFLOEYPFEGOGZVIMAFZAZMKEYEZHGCZFEHKAFZHZKGCKSETLKDKLALEERINDEFKEZQZKGAKLZHZACVAZTZZK\n",
    "\n",
    "AQTVPYGIPLIKADKVQAQGYKGANVKVGIIDTGIAASHTDLKVVGGASFVSGESYNTDGNGHGTHVAGTVAALDNTTGVLGVAPNVSLYAIKVLNSSGSGTYSAIVSGIEWATQNGLDVINMSLGGPSGSTALKQAVDKAYASGIVVVAAAGNSGSSGSQNTIGYPAKYDSVIAVGAVDSNKNRASFSSVGAELEVMAPGVSVYSTYPSNTYTSLNGTSMASPHVAGAAALILSKYPTLSASQVRNRLSSTATNLGDSFYYGKGLINVEAAAQ---------------------------------------------------------------------------------------------------------\n",
    "MMRKKSFWLGMLTAFMLVFTMAFSDSASAAQPAKNVEKDYIVGFKSGVKTASVKKDIIKESGGKVDKQFRIINAAKAKLDKEALKEVKNDPDVAYVEEDHVAHALAQTVPYGIPLIKADKVQAQGFKGANVKVAVLDTGIQASHPDLNVVGGASFVAGEAYNTDGNGHGTHVAGTVAALDNTTGVLGVAPSVSLYAVKVLNSSGSGSYSGIVSGIEWATTNGMDVINMSLGGASGSTAMKQAVDNAYAKGVVVVAAAGNSGSSGNTNTIGYPAKYDSVIAVGAVDSNSNRASFSSVGAELEVMAPGAGVYSTYPTNTYATLNGTSMASPHVAGAAALILSKHPNLSASQVRNRLSSTATYLGSSFYYGKGLINVEAAAQ\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# TODO: use alphafold2 to predict the structure of enzyme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from evodiff.collaters import D3PMCollaterMSA\n",
    "from evodiff.utils import Tokenizer\n",
    "from evodiff.losses import  D3PMCELoss,  D3PMLVBLossMSA\n",
    "from evodiff.model import MSATransformerTime\n",
    "import sys\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from sequence_models.esm import MSATransformer\n",
    "from sequence_models.constants import MSA_ALPHABET\n",
    "from evodiff.data import TRRMSADataset, A3MMSADataset\n",
    "from sequence_models.collaters import MSAAbsorbingCollater\n",
    "from sequence_models.samplers import SortishSampler, ApproxBatchSampler\n",
    "from sequence_models.losses import MaskedCrossEntropyLossMSA\n",
    "from evodiff.metrics import MaskedAccuracyMSA\n",
    "from torch.utils.data import Subset\n",
    "from sequence_models.utils import warmup, transformer_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = str(pathlib.Path.home())\n",
    "\n",
    "def get_default_args():\n",
    "    \n",
    "    args = SimpleNamespace(\n",
    "        nodes = 1, \n",
    "        gpus=1, # number of gpus per node\n",
    "        nr = 0, # ranking within the nodes\n",
    "        offset = 3, # Number of GPU devices to skip\n",
    "        dropout = 0.0,\n",
    "        weight_Decay = 0.0,\n",
    "        tie_Weights = 0,\n",
    "        task = None,\n",
    "        dataset = os.path.join(root, 'uniprot/research/msa'),\n",
    "        out_fpath = 'results',\n",
    "        state_dict = None,\n",
    "        mask = 'oadm',\n",
    "        checkpoint_frep = 120,\n",
    "        weight_save_frep = None,\n",
    "        log_freq = 1000,\n",
    "        reweighting_term = 0.001,\n",
    "        selection_type = 'MaxHamming',\n",
    "        d_embed = 1536,\n",
    "        d_hidden = 6144,\n",
    "        n_layers = 16,        \n",
    "        n_heads = 16,\n",
    "        max_batch_size = 3,\n",
    "        epochs = 100,\n",
    "        lr = 1e-4,\n",
    "        bucket_size = 1000,\n",
    "        max_tokens = 18000000,\n",
    "        warmup_steps = 15000,\n",
    "        max_square_tokens = 1000000000000000,\n",
    "        n_sequences = 64,\n",
    "        max_seq_len = 512,\n",
    "        diffusion_timesteps = 500,\n",
    "        clip = 1.0,\n",
    "        decay = 'store_true',\n",
    "    )\n",
    "    return args\n",
    "\n",
    "# 獲取默認參數\n",
    "args = get_default_args()\n",
    "\n",
    "# data path\n",
    "data_dir = os.path.join(root,'uniprot/research/msa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'B', 'Z', 'X', 'J', 'O', 'U', '-', '*', '#', '@', '!']\n",
      "Using 30 as padding index\n",
      "Using 28 as masking index\n",
      "Using 26 as gap index\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "padding_idx = tokenizer.pad_id  # PROTEIN_ALPHABET.index(PAD)\n",
    "masking_idx = tokenizer.mask_id\n",
    "gap_idx = tokenizer.gap_id\n",
    "print(tokenizer.alphabet)\n",
    "print('Using {} as padding index'.format(padding_idx))\n",
    "print('Using {} as masking index'.format(masking_idx))\n",
    "print('Using {} as gap index'.format(gap_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating lengths, depths, gap_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detergent_gap_depths: [   2  178  248    8 1292  814  105  594  139  169  260]\n",
      "detergent_depths: [ 212  604  788  382 1798  985  379 1061  537  214  529]\n",
      "detergent_lengths: [ 2 11  3  3  2 48  2 19  3  8 22]\n"
     ]
    }
   ],
   "source": [
    "detergent_files = [file for file in os.listdir(data_dir) if file.endswith('.fasta')]\n",
    "\n",
    "# the length of sequenc in .a3m\n",
    "detergent_depths = np.array([], dtype=int)\n",
    "#TODO I'm not sure but it may be the maximum ammount of gap in .a3m\n",
    "detergent_gap_depths = np.array([], dtype=int)\n",
    "# the ammount of sequences in .a3m\n",
    "detergent_lengths = np.array([], dtype=int)\n",
    "\n",
    "for filename in detergent_files:\n",
    "    input_file = os.path.join(data_dir, filename)\n",
    "    records = list(SeqIO.parse(input_file, 'fasta'))\n",
    "    depth = len(records[0].seq)\n",
    "    length = len(records)    \n",
    "    gap_depth = max([record.count('-') for record in records])\n",
    "\n",
    "    detergent_depths = np.append(detergent_depths,depth)\n",
    "    detergent_gap_depths = np.append(detergent_gap_depths,gap_depth)\n",
    "    detergent_lengths = np.append(detergent_lengths,length)\n",
    "    \n",
    "\n",
    "np.savez(os.path.join(data_dir,'detergent_depths.npz'), array=detergent_depths)\n",
    "np.savez(os.path.join(data_dir,'detergent_gap_depths.npz'), array=detergent_gap_depths)\n",
    "np.savez(os.path.join(data_dir,'detergent_lengths.npz'), array=detergent_lengths)\n",
    "\n",
    "\n",
    "print(f'detergent_gap_depths: {np.load(os.path.join(data_dir,'detergent_gap_depths.npz'))['array']}')\n",
    "print(f'detergent_depths: {np.load(os.path.join(data_dir,'detergent_depths.npz'))['array']}')\n",
    "print(f'detergent_lengths: {np.load(os.path.join(data_dir,'detergent_lengths.npz'))['array']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding detergent_depths.npz\n",
      "Excluding detergent_gap_depths.npz\n",
      "Excluding detergent_lengths.npz\n",
      "unfiltered length 11\n",
      "filter MSA depth > 64 11\n",
      "filter rows with GAPs > 512 9\n",
      "TRAIN SIZE: 9 [8 2 6 7 1 0 4 3 5]\n"
     ]
    }
   ],
   "source": [
    "from sequence_models.constants import PROTEIN_ALPHABET, PAD, GAP\n",
    "from sequence_models.utils import parse_fasta\n",
    "\n",
    "\n",
    "class MSADataset(Dataset):\n",
    "    \"\"\"Build dataset for A3M data: MSA Absorbing Diffusion model\"\"\"\n",
    "\n",
    "    def __init__(self, selection_type, n_sequences, max_seq_len, data_dir=None, min_depth=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            selection_type: str,\n",
    "                MSA selection strategy of random or MaxHamming\n",
    "            n_sequences: int,\n",
    "                number of sequences to subsample down to\n",
    "            max_seq_len: int,\n",
    "                maximum MSA sequence length\n",
    "            data_dir: str,\n",
    "                if you have a specified data directory\n",
    "        \"\"\"\n",
    "        alphabet = PROTEIN_ALPHABET\n",
    "        self.tokenizer = Tokenizer(alphabet)\n",
    "        self.alpha = np.array(list(alphabet))\n",
    "        self.gap_idx = self.tokenizer.alphabet.index(GAP)\n",
    "\n",
    "        # Get npz_data dir\n",
    "        if data_dir is not None:\n",
    "            self.data_dir = data_dir\n",
    "        else:\n",
    "            raise FileNotFoundError(data_dir)\n",
    "        \n",
    "        [print(\"Excluding\", x) for x in os.listdir(self.data_dir) if x.endswith('.npz')]\n",
    "        all_files = [x for x in os.listdir(self.data_dir) if x.endswith('.a3m')]\n",
    "        all_files = sorted(all_files)\n",
    "        print(\"unfiltered length\", len(all_files))\n",
    "\n",
    "\n",
    "        ## Filter based on depth (keep > 64 seqs/MSA)\n",
    "        if not os.path.exists(os.path.join(data_dir,'detergent_lengths.npz')):\n",
    "            raise Exception(f\"Missing detergent_lenths.npz in {data_dir}\")\n",
    "        if not os.path.exists(os.path.join(data_dir,'detergent_depths.npz')):\n",
    "            #get_msa_depth_openfold(data_dir, sorted(all_files), 'openfold_depths.npz')\n",
    "            raise Exception(f\"Missing detergent_depths.npz in {data_dir}\")\n",
    "        if min_depth is not None: # reindex, filtering out MSAs < min_depth\n",
    "            _depths = np.load(os.path.join(data_dir,'detergent_depths.npz'))['array']\n",
    "            depths = pd.DataFrame(_depths, columns=['depth'])\n",
    "            depths = depths[depths['depth'] >= min_depth]\n",
    "            keep_idx = depths.index\n",
    "\n",
    "            _lengths = np.load(os.path.join(data_dir,'detergent_lengths.npz'))['array']\n",
    "            lengths = np.array(_lengths)[keep_idx]\n",
    "            all_files = np.array(all_files)[keep_idx]\n",
    "            print(\"filter MSA depth > 64\", len(all_files))\n",
    "\n",
    "\n",
    "        # Re-filter based on high gap-contining rows\n",
    "        if not os.path.exists(os.path.join(data_dir,'detergent_gap_depths.npz')):\n",
    "            #get_sliced_gap_depth_openfold(data_dir, all_files, 'openfold_gap_depths.npz', max_seq_len=max_seq_len)\n",
    "            raise Exception(f\"Missing detergent_gap_depths.npz in {data_dir}\")\n",
    "        _gap_depths = np.load(os.path.join(data_dir,'detergent_gap_depths.npz'))['array']\n",
    "        gap_depths = pd.DataFrame(_gap_depths, columns=['gapdepth'])\n",
    "        gap_depths = gap_depths[gap_depths['gapdepth'] >= min_depth]\n",
    "        filter_gaps_idx = gap_depths.index\n",
    "        lengths = np.array(lengths)[filter_gaps_idx]\n",
    "        all_files = np.array(all_files)[filter_gaps_idx]\n",
    "        print(\"filter rows with GAPs > 512\", len(all_files))\n",
    "\n",
    "\n",
    "        self.filenames = all_files  # IDs of samples to include\n",
    "        self.lengths = lengths # pass to batch sampler\n",
    "        self.n_sequences = n_sequences\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.selection_type = selection_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "\n",
    "        def read_files(data_dir, filename):\n",
    "            \"\"\"\n",
    "            inputs:\n",
    "                data_dir : path to directory with data\n",
    "                filename: MSA name\n",
    "\n",
    "            outputs:\n",
    "                path: path to .a3m file\n",
    "            \"\"\"\n",
    "            if os.path.exists(os.path.join(data_dir, filename)):\n",
    "                path = os.path.join(data_dir, filename)\n",
    "            else:\n",
    "                raise Exception(\"Missing filepaths\")\n",
    "            return path\n",
    "\n",
    "\n",
    "        path = read_files(self.data_dir, filename)\n",
    "        parsed_msa = parse_fasta(path)\n",
    "\n",
    "        aligned_msa = [[char for char in seq if (char.isupper() or char == '-') and not char == '.'] for seq in parsed_msa]\n",
    "        aligned_msa = [''.join(seq) for seq in aligned_msa]\n",
    "\n",
    "        tokenized_msa = [self.tokenizer.tokenizeMSA(seq) for seq in aligned_msa]\n",
    "        tokenized_msa = np.array([l.tolist() for l in tokenized_msa])\n",
    "        msa_seq_len = len(tokenized_msa[0])\n",
    "\n",
    "        if msa_seq_len > self.max_seq_len:\n",
    "            slice_start = np.random.choice(msa_seq_len - self.max_seq_len + 1)\n",
    "            seq_len = self.max_seq_len\n",
    "        else:\n",
    "            slice_start = 0\n",
    "            seq_len = msa_seq_len\n",
    "\n",
    "        # Slice to 512\n",
    "        sliced_msa_seq = tokenized_msa[:, slice_start: slice_start + self.max_seq_len]\n",
    "        anchor_seq = sliced_msa_seq[0]  # This is the query sequence in MSA\n",
    "\n",
    "        # slice out all-gap rows\n",
    "        sliced_msa = [seq for seq in sliced_msa_seq if (list(set(seq)) != [self.gap_idx])]\n",
    "        msa_num_seqs = len(sliced_msa)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if msa_num_seqs < self.n_sequences:\n",
    "            print(\"before for len\", len(sliced_msa_seq))\n",
    "            print(\"msa_num_seqs < self.n_sequences should not be called\")\n",
    "            print(\"tokenized msa shape\", tokenized_msa.shape)\n",
    "            print(\"tokenized msa depth\", len(tokenized_msa))\n",
    "            print(\"sliced msa depth\", msa_num_seqs)\n",
    "            print(\"used to set slice\")\n",
    "            print(\"msa_seq_len\", msa_seq_len)\n",
    "            print(\"self max seq len\", self.max_seq_len)\n",
    "            print(slice_start)\n",
    "            import pdb; pdb.set_trace()\n",
    "            output = np.full(shape=(self.n_sequences, seq_len), fill_value=self.tokenizer.pad_id)\n",
    "            output[:msa_num_seqs] = sliced_msa\n",
    "            raise Exception(\"msa num_seqs < self.n_sequences, indicates dataset not filtered properly\")\n",
    "        elif msa_num_seqs > self.n_sequences:\n",
    "            if self.selection_type == 'random':\n",
    "                random_idx = np.random.choice(msa_num_seqs - 1, size=self.n_sequences - 1, replace=False) + 1\n",
    "                anchor_seq = np.expand_dims(anchor_seq, axis=0)\n",
    "                output = np.concatenate((anchor_seq, np.array(sliced_msa)[random_idx.astype(int)]), axis=0)\n",
    "            elif self.selection_type == \"MaxHamming\":\n",
    "                output = [list(anchor_seq)]\n",
    "                msa_subset = sliced_msa[1:]\n",
    "                msa_ind = np.arange(msa_num_seqs)[1:]\n",
    "                random_ind = np.random.choice(msa_ind)\n",
    "                random_seq = sliced_msa[random_ind]\n",
    "                output.append(list(random_seq))\n",
    "                random_seq = np.expand_dims(random_seq, axis=0)\n",
    "                msa_subset = np.delete(msa_subset, (random_ind - 1), axis=0)\n",
    "                m = len(msa_ind) - 1\n",
    "                distance_matrix = np.ones((self.n_sequences - 2, m))\n",
    "\n",
    "                for i in range(self.n_sequences - 2):\n",
    "                    curr_dist = cdist(random_seq, msa_subset, metric='hamming')\n",
    "                    curr_dist = np.expand_dims(np.array(curr_dist), axis=0)  # shape is now (1,msa_num_seqs)\n",
    "                    distance_matrix[i] = curr_dist\n",
    "                    col_min = np.min(distance_matrix, axis=0)  # (1,num_choices)\n",
    "                    max_ind = np.argmax(col_min)\n",
    "                    random_ind = max_ind\n",
    "                    random_seq = msa_subset[random_ind]\n",
    "                    output.append(list(random_seq))\n",
    "                    random_seq = np.expand_dims(random_seq, axis=0)\n",
    "                    msa_subset = np.delete(msa_subset, random_ind, axis=0)\n",
    "                    distance_matrix = np.delete(distance_matrix, random_ind, axis=1)\n",
    "        else:\n",
    "            output = sliced_msa\n",
    "\n",
    "        output = [''.join(seq) for seq in self.alpha[output]]\n",
    "        return output\n",
    "\n",
    "min_depth = args.n_sequences\n",
    "\n",
    "dataset = MSADataset(args.selection_type, args.n_sequences, args.max_seq_len, data_dir=data_dir, min_depth=min_depth)\n",
    "train_size = len(dataset)\n",
    "\n",
    "random_ind = np.random.choice(train_size, size=(train_size - 10000 if train_size>1000 else train_size), replace=False)\n",
    "print(\"TRAIN SIZE:\", train_size, random_ind)\n",
    "\n",
    "\n",
    "if args.mask == 'oadm':\n",
    "    collater = MSAAbsorbingCollater(alphabet=MSA_ALPHABET, num_seqs=2)\n",
    "    diffusion_timesteps = None # Not input to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.cuda.set_device(args.offset)\n",
    "device = torch.device('cuda:' + str(args.offset))\n",
    "\n",
    "selection_type = args.selection_type\n",
    "min_depth = args.n_sequences # Will filter out sequences smaller than this number\n",
    "if hasattr(args, 'clip'):\n",
    "    clip = args.clip\n",
    "else:\n",
    "    # 設置一個默認值，或者處理沒有 clip 屬性的情況\n",
    "    clip = None  # 或其他適當的默認值\n",
    "\n",
    "ptjob = False\n",
    "\n",
    "# build datasets, samplers, and loaders\n",
    "\n",
    "ds_train = Subset(dataset, random_ind)\n",
    "\n",
    "# if args.dataset == os.path.join(root, 'uniprot/research/msa'):\n",
    "#metadata = np.load(data_dir + config['dataset'] + '_lengths.npz')['ells']\n",
    "metadata = np.array(dataset.lengths)\n",
    "train_idx = ds_train.indices\n",
    "#print(train_idx)\n",
    "len_train = metadata[train_idx]\n",
    "\n",
    "len_train = np.minimum(len_train, args.max_seq_len)\n",
    "\n",
    "train_sortish_sampler = SortishSampler(len_train, args.bucket_size)\n",
    "train_sampler = ApproxBatchSampler(train_sortish_sampler, args.max_tokens, args.max_batch_size, len_train,\n",
    "                                    max_square_tokens=args.max_square_tokens, msa_depth=args.n_sequences)\n",
    "dl_train = DataLoader(dataset=ds_train,  batch_sampler=train_sampler, collate_fn=collater, num_workers=8)\n",
    "\n",
    "\n",
    "# if rank == 0: 2024/07/23 still not figure out what it is\n",
    "val_ind = np.delete(np.arange(train_size), random_ind)\n",
    "ds_valid = Subset(dataset, val_ind)\n",
    "valid_idx = ds_valid.indices\n",
    "len_valid = metadata[valid_idx]\n",
    "len_valid = np.minimum(len_valid, args.max_seq_len)\n",
    "\n",
    "valid_sortish_sampler = SortishSampler(len_valid, args.bucket_size, num_replicas=1, rank=0)\n",
    "valid_sampler = ApproxBatchSampler(valid_sortish_sampler, args.max_tokens, args.max_batch_size, len_valid,\n",
    "                                    max_square_tokens=args.max_square_tokens, msa_depth=args.n_sequences)\n",
    "\n",
    "dl_valid = DataLoader(dataset=ds_valid,\n",
    "                        batch_sampler=valid_sampler,\n",
    "                        collate_fn=collater,\n",
    "                        num_workers=8)\n",
    "\n",
    "# Initiate model\n",
    "if args.mask == 'oadm':\n",
    "    model = MSATransformer(args.d_embed, args.d_hidden, args.n_layers, args.n_heads, use_ckpt=True, n_tokens=len(MSA_ALPHABET),\n",
    "                            padding_idx=padding_idx, mask_idx=masking_idx).cuda()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "if args.decay:\n",
    "    scheduler = LambdaLR(optimizer, transformer_lr(args.warmup_steps))\n",
    "else:\n",
    "    scheduler = LambdaLR(optimizer, warmup(args.warmup_steps))\n",
    "scaler = GradScaler()\n",
    "\n",
    "outputs = os.listdir(args.out_fpath)\n",
    "\n",
    "if len(outputs) > 0:\n",
    "    last_epoch = -1\n",
    "    for output in outputs:\n",
    "        if 'checkpoint' in output:\n",
    "            epoch = int(output.split('checkpoint')[-1][:-4])\n",
    "            if epoch > last_epoch:\n",
    "                args.state_dict = args.out_fpath + output\n",
    "                last_epoch = epoch\n",
    "if args.state_dict is not None:\n",
    "    print('Loading weights from ' + args.state_dict + '...')\n",
    "    sd = torch.load(args.state_dict, map_location=torch.device('cpu'))\n",
    "    msd = sd['model_state_dict']\n",
    "    msd = {k.split('module.')[1]: v for k, v in msd.items()}\n",
    "    model.load_state_dict(msd)\n",
    "    optimizer.load_state_dict(sd['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(sd['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(sd['scaler_state_dict']),\n",
    "    initial_epoch = sd['epoch'] + 1\n",
    "    total_steps = sd['step']\n",
    "    total_tokens = sd['tokens']\n",
    "else:\n",
    "    initial_epoch = 0\n",
    "    total_steps = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if args.mask == 'oadm':\n",
    "    loss_func = MaskedCrossEntropyLossMSA(ignore_index=padding_idx)\n",
    "\n",
    "\n",
    "accu_func = MaskedAccuracyMSA()\n",
    "# if rank == 0: 2024/07/23 still not figure out what it is\n",
    "# with open(args.config_fpath, 'r') as f_from:\n",
    "#     with open(args.out_fpath + \"config.json\", \"w\") as f_to:\n",
    "#         f_to.write(f_from.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, nll_loss, accu, n_tokens, n_seqs, n_processed\n\u001b[1;32m    195\u001b[0m n_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m model parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m n_parameters)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(initial_epoch, epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rank' is not defined"
     ]
    }
   ],
   "source": [
    "def epoch(model, e, split, current_step=0, current_tokens=0):\n",
    "    start_time = datetime.now()\n",
    "    if split == 'train':\n",
    "        loader = dl_train\n",
    "        t = 'Training:'\n",
    "    elif split == 'valid':\n",
    "        loader = dl_valid\n",
    "        t = 'Validating:'\n",
    "    else:\n",
    "        # loader = dl_test\n",
    "        t = \"Testing\"\n",
    "    ardm_losses = []\n",
    "    nll_losses = []\n",
    "    accus = []\n",
    "    ns = []\n",
    "    num_seqs = []\n",
    "    chunk_time = datetime.now()\n",
    "    weight_chunk_time = datetime.now()\n",
    "    n_seen = 0\n",
    "    tokens_trained = current_tokens\n",
    "    if split == 'train':\n",
    "        n_total = len(ds_train)\n",
    "    elif split == 'valid':\n",
    "        n_total = len(ds_valid)\n",
    "    # else:\n",
    "    #     n_total = len(ds_test)\n",
    "    for i, batch in enumerate(loader):\n",
    "        if split == 'train' and i == 1 and e == initial_epoch and args.state_dict is not None:\n",
    "            optimizer.load_state_dict(sd['optimizer_state_dict'])\n",
    "            scheduler.load_state_dict(sd['scheduler_state_dict'])\n",
    "            scaler.load_state_dict(sd['scaler_state_dict'])\n",
    "        ardm_loss, nll_loss, new_accu, new_n, new_seqs, new_processed = step(model, batch, split)\n",
    "\n",
    "        if split == 'train':\n",
    "            dist.reduce(ardm_loss, 0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(nll_loss, 0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(new_accu, 0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(new_n, 0, op=dist.ReduceOp.SUM)\n",
    "            dist.reduce(new_seqs, 0, op=dist.ReduceOp.SUM)\n",
    "        ardm_losses.append(ardm_loss.item())\n",
    "        nll_losses.append(nll_loss.item())\n",
    "        accus.append(new_accu.item())\n",
    "        ns.append(new_n.item())\n",
    "        num_seqs.append(new_seqs.item())\n",
    "        n_seen += new_seqs.item()\n",
    "        total_n = sum(ns)\n",
    "        total_s = sum(num_seqs)\n",
    "        rloss_ardm = sum(ardm_losses) / total_n\n",
    "        rloss_nll = sum(nll_losses) / total_n\n",
    "        raccu = sum(accus) / total_n\n",
    "\n",
    "        if split == 'train':\n",
    "            # writer.add_scalar(\"Loss/train\", rloss, e)\n",
    "            # writer.add_scalar(\"Acc/train\", raccu, e)\n",
    "            nsteps = current_step + i + 1\n",
    "            tokens_trained += new_processed.item()\n",
    "        else:\n",
    "            # writer.add_scalar(\"Loss/valid\", rloss, e)\n",
    "            # writer.add_scalar(\"Acc/valid\", raccu, e)\n",
    "            nsteps = i\n",
    "        if rank == 0:\n",
    "            if ptjob:\n",
    "                end = '\\n'\n",
    "                start = ''\n",
    "            else:\n",
    "                start = ''\n",
    "                end = '\\n'\n",
    "            print(\n",
    "                start + '%s Epoch %d of %d Step %d Example %d of %d ardm_loss = %.4f nll_loss = %.4f accu = %.4f'\n",
    "                % (t, e + 1, epochs, nsteps, n_seen, n_total, rloss_ardm, rloss_nll, raccu),\n",
    "                end=end)\n",
    "            print('\\n')\n",
    "\n",
    "        if split == 'train':\n",
    "            ardm_losses = ardm_losses[-999:]\n",
    "            nll_losses = nll_losses[-999:]\n",
    "            accus = accus[-999:]\n",
    "            ns = ns[-999:]\n",
    "            num_seqs = num_seqs[-999:]\n",
    "            if nsteps % args.log_freq == 0:\n",
    "                if rank == 0:\n",
    "                    with open(args.out_fpath + 'metrics_train.csv', 'a') as f:\n",
    "                        #f.write(','.join(\n",
    "                        #    [str(rloss_ardm), str(rloss_nll), str(raccu), str(int(current_tokens)), str(current_step)]))\n",
    "                        #f.write('\\n')  # Can add for train too\n",
    "                        f.write(','.join(\n",
    "                            [str(rloss_ardm), str(rloss_nll), str(raccu), str(int(current_tokens)),\n",
    "                                str(nsteps), str(e)]))\n",
    "                        f.write('\\n')\n",
    "            if datetime.now() - chunk_time > timedelta(minutes=args.checkpoint_freq):\n",
    "                if rank == 0:\n",
    "                    print('Training complete in ' + str(datetime.now() - chunk_time))\n",
    "                    with torch.no_grad():\n",
    "                        if rank == 0:\n",
    "                            ckpt_fpath = args.out_fpath + 'checkpoint%d.tar' % nsteps\n",
    "                            torch.save({\n",
    "                                'step': nsteps,\n",
    "                                'tokens': tokens_trained,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'scaler_state_dict': scaler.state_dict(),\n",
    "                                'epoch': e,\n",
    "                                # 'amp_state_dict': amp.state_dict()\n",
    "                            }, ckpt_fpath)\n",
    "                            _ = epoch(model, e, split='valid', current_step=nsteps, current_tokens=tokens_trained)\n",
    "                    chunk_time = datetime.now()\n",
    "                    weight_chunk_time = datetime.now()\n",
    "            elif datetime.now() - weight_chunk_time > timedelta(minutes=args.weight_save_freq):\n",
    "                if rank == 0:\n",
    "                    print('Saving weights ' + str(datetime.now() - chunk_time))\n",
    "                    with torch.no_grad():\n",
    "                        if rank == 0:\n",
    "                            ckpt_fpath = args.out_fpath + 'checkpoint%d.tar' % nsteps\n",
    "                            torch.save({\n",
    "                                'step': nsteps,\n",
    "                                'tokens': tokens_trained,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                                'scaler_state_dict': scaler.state_dict(),\n",
    "                                'epoch': e,\n",
    "                                # 'amp_state_dict': amp.state_dict()\n",
    "                            }, ckpt_fpath)\n",
    "                    weight_chunk_time = datetime.now()\n",
    "    if split == 'valid':\n",
    "        if rank == 0:\n",
    "            with open(args.out_fpath + 'metrics.csv', 'a') as f:\n",
    "                # f.write(','.join(\n",
    "                #     [str(rloss_ardm), str(rloss_nll), str(raccu), str(int(current_tokens)), str(current_step)]))\n",
    "                # f.write('\\n')  # Can add for train too\n",
    "                f.write(','.join(\n",
    "                    [str(rloss_ardm), str(rloss_nll), str(raccu), str(int(current_tokens)),\n",
    "                        str(current_step), str(e)]))\n",
    "                f.write('\\n')\n",
    "        print('Validation complete in ' + str(datetime.now() - start_time))\n",
    "    print('Epoch complete in ' + str(datetime.now() - start_time))\n",
    "    return i, tokens_trained\n",
    "\n",
    "def step(model, batch, split):\n",
    "    if args.mask == 'blosum' or args.mask == 'random':\n",
    "        src, src_one_hot, timestep, tgt, tgt_one_hot, Q, Q_prod, q = batch\n",
    "        src_one_hot = src_one_hot.to(device)\n",
    "        tgt_one_hot = tgt_one_hot.to(device)\n",
    "        q = q.to(device)\n",
    "        Q = Q.to(device)\n",
    "        Q_prod = Q_prod.to(device)\n",
    "        timestep = timestep.to(device)\n",
    "    else:\n",
    "        src, tgt, mask = batch\n",
    "        mask = mask.to(device)\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "    input_mask = (src != masking_idx).float()\n",
    "    nonpad_mask = (src != padding_idx).float()\n",
    "    if args.mask == 'blosum' or args.mask == 'random':\n",
    "        n_tokens = nonpad_mask.sum()\n",
    "    else:\n",
    "        n_tokens = mask.sum()\n",
    "    if n_tokens == 0:\n",
    "        raise ValueError(\"N TOKENS IN STEP IS 0!!\")\n",
    "    n_processed = input_mask.sum()\n",
    "\n",
    "    if split == 'train':\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if args.mask == 'blosum' or args.mask == 'random':\n",
    "        outputs = model(src, timestep)\n",
    "        lvb_loss = loss_func1(src_one_hot, q, outputs, tgt, tgt_one_hot, nonpad_mask, timestep, Q, Q_prod)\n",
    "        ce_loss = loss_func2(outputs, tgt, nonpad_mask)\n",
    "        lvb_loss = lvb_loss.to(torch.float32)\n",
    "        ce_loss = ce_loss.to(torch.float32)\n",
    "        nll_loss = ce_loss * n_tokens\n",
    "        accu = accu_func(outputs, tgt, nonpad_mask) * n_tokens\n",
    "        loss = (lvb_loss + _lambda * ce_loss) * n_tokens\n",
    "    elif args.mask == 'oadm':\n",
    "        outputs = model(src)\n",
    "        ce_loss, nll_loss = loss_func(outputs, tgt, mask, nonpad_mask)\n",
    "        loss = ce_loss\n",
    "        accu = accu_func(outputs, tgt, mask) * n_tokens\n",
    "\n",
    "    if split == 'train':\n",
    "        scaler.scale(loss).backward()\n",
    "        _ = clip_grad_norm_(model.parameters(), clip)\n",
    "        scaler.step(optimizer)\n",
    "        scale = scaler.get_scale()\n",
    "        scaler.update()\n",
    "        skip_scheduler = (scale > scaler.get_scale())\n",
    "        if not skip_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    n_seqs = torch.tensor(len(src), device=device)\n",
    "    return loss, nll_loss, accu, n_tokens, n_seqs, n_processed\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters())\n",
    "if rank == 0:\n",
    "    print('%d model parameters' % n_parameters)\n",
    "for e in range(initial_epoch, epochs):\n",
    "    print(\"epoch: \", e + 1, rank)\n",
    "    s, t = epoch(model, e, split='train', current_step=total_steps, current_tokens=total_tokens)\n",
    "    total_steps += s\n",
    "    total_tokens += t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MSATransformer(256,128,16,16)\n",
    "\n",
    "checkpoint = MSA_OA_DM_MAXSUB()\n",
    "model, collater, tokenizer, scheme = checkpoint\n",
    "\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529\n",
      "torch.Size([1, 1, 529])\n",
      "torch.Size([1, 1, 529, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chialun/.conda/envs/evodiff/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 12.9114, -35.5368,  -8.6801,  ...,   8.7947,  31.8852,  34.8907],\n",
      "          [ 42.2695, -21.8901, -11.5830,  ...,   8.8298,  36.4136,  30.8669],\n",
      "          [ 26.6929, -29.1680,  -5.0770,  ...,   8.7894,  40.7508,  18.5738],\n",
      "          ...,\n",
      "          [ 19.7565, -28.4536, -15.5611,  ...,   8.8328,  33.9661,  28.4636],\n",
      "          [ 15.6928, -32.2167, -13.9613,  ...,   8.9582,  30.1659,  28.8393],\n",
      "          [ 10.9344, -43.0428,   1.6987,  ...,   9.0806,  41.4534,  24.4211]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 10,  9, 15,  5,  9, 15,\n",
      "           7, 15,  0,  0,  6,  0, 16, 11,  0,  3, 13, 17,  8, 11, 15,  4, 17,\n",
      "          19, 15, 15, 19,  0, 13, 16,  8, 19, 12,  9, 17,  4, 11,  6,  5, 10,\n",
      "           0,  5,  4, 11, 14, 17,  5, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 16,  2, 16,  9,  5,  9,  2, 19, 18, 19, 13,  7,  9, 12,  2,  9,\n",
      "           0, 14, 11,  5,  5, 11, 17, 18,  0, 16, 14, 17, 15, 12,  4, 11, 15,\n",
      "          16,  3, 17, 14,  5,  3, 13,  9,  0, 13, 13, 17, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26,  3,  3,  7,  7,  0,  7, 16,  5,  8, 12,  8, 17, 11,\n",
      "           9,  7,  5,  6, 15,  6,  5,  5, 12, 16,  7, 14, 19, 17,  0,  5,  7,\n",
      "          10, 12,  3,  8, 17,  0, 15,  9, 16, 16,  7,  5,  0, 12,  6,  8,  5,\n",
      "          15, 12, 10,  0,  2, 17,  7,  9, 11, 17,  3,  5, 26, 26, 26, 16, 12,\n",
      "           9, 15,  5,  9,  0, 16,  9, 17, 11, 18,  4, 15,  0,  0,  7, 16, 18,\n",
      "           0,  5,  5,  9,  2, 12, 16, 15, 19, 12,  6,  2, 15,  9,  0,  5,  0,\n",
      "           6, 15,  9, 15, 16, 13,  5, 15,  0, 13,  4, 11,  0, 13,  4, 12, 10,\n",
      "           5, 17, 12, 16, 16, 15,  1,  5,  3,  5, 16, 19, 13,  3,  8,  5,  7,\n",
      "          19, 10, 19, 15,  4, 15,  5, 11,  8,  0,  9, 16, 11, 12,  9,  2, 12,\n",
      "           4,  2,  7,  0,  9, 16,  5, 15, 15,  9, 17, 17,  2, 12,  4,  5, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,  2, 11,  2,\n",
      "           5,  9, 17, 15, 14,  1, 15,  0,  8,  4,  5,  8, 16,  7, 14,  2,  2,\n",
      "          19, 11, 18, 11,  6,  9,  2,  3, 17, 11, 13, 17,  9,  5,  7, 14, 15,\n",
      "           7,  4,  0, 15,  2, 12, 17, 15, 17, 19, 14, 13,  6,  0, 11, 14,  9,\n",
      "           8,  9, 13,  5,  9, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "          26, 26]]])\n",
      "tensor([26, 26, 21, 26, 26, 26, 26, 26, 26, 21, 26,  7, 21, 21, 26,  4, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 26, 21, 21, 21, 16, 26, 26, 21, 21, 26,  4, 21,\n",
      "        21, 26, 29, 21, 26, 26, 29, 21, 21, 26, 21, 29, 21, 30, 21, 21, 26, 21,\n",
      "        23, 26,  7, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 21, 21, 21, 21,\n",
      "        21, 30, 21, 21, 21, 26,  4, 21, 23, 26, 21, 21,  4, 21, 21, 20, 23, 21,\n",
      "        29, 16, 23, 21, 21, 21, 21, 21, 21, 21, 29, 21, 21,  4, 26, 30,  4, 21,\n",
      "        21, 21, 21, 21, 21,  7, 26, 26, 26, 26, 26, 26, 26, 21, 26, 21, 21, 21,\n",
      "        21, 21, 21, 21, 23, 26, 21, 30, 26, 20, 30, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21,  4, 26, 21, 30, 21, 26, 21, 21, 26, 16, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 29, 21, 21, 21, 26, 21, 21, 26, 21, 30, 20, 21,\n",
      "        21, 30, 26, 26, 26, 21, 21, 30, 21, 30, 21, 21, 21, 26, 26, 23, 26, 21,\n",
      "        21, 21, 21, 21, 21, 23, 21, 30, 26, 30, 21, 21, 21, 21,  4, 21,  5, 21,\n",
      "        21,  4, 21, 23, 21, 21, 21, 30, 16, 21, 21, 30, 21, 21, 21,  4, 26, 21,\n",
      "        21, 21, 21, 30, 21, 21, 21, 21, 21, 21, 21, 30, 21, 30, 29,  4, 21, 21,\n",
      "        26, 30, 21, 30,  7, 16, 21, 21,  0, 21, 21, 26, 21, 30, 21, 29, 21, 26,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,  0, 26, 21, 21, 21, 26,\n",
      "        26, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 21, 20, 21,\n",
      "        21,  4, 26, 21, 23,  0, 21, 21, 26, 21, 20, 29, 21, 21, 26, 21, 21, 21,\n",
      "        20, 23, 23, 21,  4, 21, 21, 16, 29, 21, 26,  4, 23, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 26, 21, 21, 16, 26, 21, 21, 21, 29, 26, 21, 21, 30, 21,\n",
      "        21, 21, 26, 26, 26, 26, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 21, 21,\n",
      "        26, 26, 26, 26, 26, 26, 21, 21, 26, 26, 26, 26, 21, 26, 26, 21, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26, 26, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26, 26, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 21, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26, 21, 26, 26, 26, 26, 26, 21, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 21, 26, 26, 21, 26, 26, 26, 26, 26, 21, 26, 26, 26, 21,\n",
      "        26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 21, 26,\n",
      "        26, 21, 21, 26, 26, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 21,\n",
      "        26, 26, 21, 26, 26, 26, 26], device='cuda:0')\n",
      "Encoded input shape: torch.Size([1, 1, 529])\n",
      "Prediction shape: torch.Size([1, 1, 529])\n",
      "Correlation coefficient: 0.3363\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "src = '-----------MLSGLSISAAHATNAEQVKNSFVYSSYAQTKYPLVFNHGMAGFNRVG-----------TDTLGLDYWYQILPDLARNGGNVWATRVSPFNSTEVRGEQLAQQV---------EEIIAITGKPKVNLIGHSHGGPTIRYVAGIMPEKVASLTTIGAPHKGSPMADVILNVEG---TPLSGLATLVNWFSAAITWAGGLDPTSYPHDSLAGAHSLSTQGSAQFNAQFPMGVPTTSCGEGTYQEKGIYMYSFSGNKALTNPLDPFDIALTGSSLVVDPFG---------------DNDGLVSRCSAKFGKTIRDDYNWNHLDEVNQVLGIRSIFASDPVSVYRQHANRLKLQGL-----------------------------------------------------------------------------------------------------------------------------------------------------------------------'\n",
    "print(len(src))\n",
    "encode = torch.tensor([[tokenizer.tokenizeMSA(src)]])\n",
    "\n",
    "print(encode.shape)\n",
    "pred = model.forward(encode.to(device))\n",
    "print(pred.shape)\n",
    "print(pred)\n",
    "\n",
    "\n",
    "\n",
    "batchsize, length, depth, tokens = pred.shape\n",
    "\n",
    "_, p = torch.max(torch.nn.functional.softmax(pred, dim=-1), -1)\n",
    "print(encode)\n",
    "print(np.squeeze(p))\n",
    "\n",
    "print(\"Encoded input shape:\", encode.shape)\n",
    "print(\"Prediction shape:\", p.shape)\n",
    "\n",
    "\n",
    "encode_np = encode.cpu().numpy().squeeze()\n",
    "p_np = p.cpu().numpy().squeeze()\n",
    "correlation = np.corrcoef(encode_np, p_np)[0, 1]\n",
    "print(f\"Correlation coefficient: {correlation:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
